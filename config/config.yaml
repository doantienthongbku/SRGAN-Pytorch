output_dir: 'runs/train'
log_dir: 'runs/log'
test_dir: 'runs/test'
infer_dir: 'runs/infer'
device: 'cuda:1'
seed: 42
# Turning on when the image size does not change during training can speed up training
cudnn_benchmark: True
# When evaluating the performance of the SR model, whether to verify only the Y channel image data
only_test_y_channel: True

dataset:
    # Dataset name
    name: "DIV2K"
    image_format: "png"
    # Dataset address after preprocessing (cropping, scaling, etc.)
    train_hr_images_dir: "/home/lake/hehehe/datasets/DIV2K_train_HR"
    valid_hr_images_dir: "/home/lake/hehehe/datasets/DIV2K_valid_HR"
    # test dataset 
    test_hr_images_dir: "../validation_data/BSDS100/GTmod12"
    test_lr_images_dir: "../validation_data/BSDS100/LRbicx4"
    # Dataset parameters
    crop_size: 96
    upscale_factor: 4
    batch_size: 128
    num_workers: 4
    # Data augmentation
    augment: True
    # Data normalization
    normalize: True

model:
    # Model architecture name
    d_arch_name: "discriminator"
    g_arch_name: "srresnet_x4"
    # Model arch config
    in_channels: 3
    out_channels: 3
    channels: 64
    num_rcb:  16
    # Experiment name, easy to save weights and log files
    exp_name: "SRGAN_x4-DIV2K"

train:
    # The address to load the pretrained model
    pretrained_d_model_weights_path: ""
    pretrained_g_model_weights_path: ""
    # Incremental training and migration training
    resume: False
    resume_d_model_weights_path: ""
    resume_g_model_weights_path: ""
    # Training epochs
    num_epochs: 20
    # Loss function weight
    pixel_weight: 1.0
    content_weight: 1.0
    adversarial_weight: 0.001
    # Optimizer parameter
    model_lr: 1e-4
    model_betas: (0.9, 0.999)
    model_eps: 1e-8
    model_weight_decay: 1e-8
    # Dynamically adjust the learning rate policy
    lr_scheduler_step_size: 10
    lr_scheduler_gamma: 0.1
    # How many iterations to print the training result
    train_print_frequency: 100
    valid_print_frequency: 100
    # Feature extraction layer parameter configuration use for content loss
    feature_model_extractor_node: "features.35"
    feature_model_normalize_mean: [0.485, 0.456, 0.406]
    feature_model_normalize_std: [0.229, 0.224, 0.225]

test:
    g_model_weights_path: "runs/train/g_model_best.pth"
